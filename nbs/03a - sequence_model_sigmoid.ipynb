{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path(\"../rsna_data/\")\n",
    "embspath = Path(datapath/\"cnn_embs\")\n",
    "train_df = pd.read_csv(datapath/'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(embspath.ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embeddings & Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "folddir = embspath/f'full_512_ALL_FROM_FOLD{fold}'; list(folddir.ls())\n",
    "# folddir = embspath/f'full_EFFNETB3_512_ALL_FROM_FOLD{fold}'; list(folddir.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.cat([torch.load(folddir/f\"xresnet34_embeddings_{o}.pth\") for o in [\"part0\", \"part1\", \"part2\", \"finalpart\"]])\n",
    "# embeddings = torch.cat([torch.load(folddir/f\"effb3_embeddings_{o}.pth\") for o in [\"part0\", \"part1\", \"part2\", \"finalpart\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.load(folddir/\"preds.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = torch.load(folddir/\"files.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape, preds.shape, len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add zero for padded input idx\n",
    "input_pad_idx = len(embeddings)\n",
    "embeddings = torch.cat([embeddings, torch.zeros_like(embeddings[:1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for effnet\n",
    "embeddings = embeddings.squeeze(-1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[input_pad_idx], embeddings.shape, input_pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = datapath/'metadata'\n",
    "metadata_files = get_files(metadata_path, extensions=\".csv\")\n",
    "metadf = pd.concat([pd.read_csv(o) for o in metadata_files]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metadf['StudyInstanceUID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scaler(o): return (o - min(o))/(max(o) - min(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pos = metadf.groupby('StudyInstanceUID')['ImagePositionPatient2'].apply(minmax_scaler)\n",
    "metadf.loc[:,'scaled_position'] = scaled_pos.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feat_cols = ['scaled_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isnan(metadf[meta_feat_cols]).sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std = metadf[meta_feat_cols].agg(['mean', 'std']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std_dict = dict(zip(mean_std.index, mean_std.values.tolist())); mean_std_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scaler for training\n",
    "for c in mean_std_dict: metadf[c] = (metadf[c] - mean_std_dict[c][0]) / mean_std_dict[c][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feats_dict = dict(zip(metadf['SOPInstanceUID'], metadf[meta_feat_cols].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meta_feats_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fold Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embeddings = []\n",
    "for o in files:\n",
    "    sopid = o.stem.split(\"_\")[1]\n",
    "    meta_embeddings.append(meta_feats_dict[sopid])\n",
    "meta_embeddings = np.vstack(meta_embeddings)\n",
    "meta_embeddings= tensor(meta_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_preds = True\n",
    "if use_preds:\n",
    "    meta_embeddings = torch.cat([meta_embeddings, preds[:,1].view(-1,1).float()],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embeddings.shape, type(meta_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embeddings = torch.cat([meta_embeddings, torch.zeros_like(meta_embeddings[:1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape, meta_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embeddings = torch.cat([embeddings, meta_embeddings], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_targets = ['pe_present_on_image']\n",
    "exam_targets = [\n",
    "    'negative_exam_for_pe', # exam level\n",
    "    'rv_lv_ratio_gte_1', # exam level\n",
    "    'rv_lv_ratio_lt_1', # exam level\n",
    "    'leftsided_pe', # exam level\n",
    "    'chronic_pe', # exam level\n",
    "    'rightsided_pe', # exam level\n",
    "    'acute_and_chronic_pe', # exam level\n",
    "    'central_pe', # exam level\n",
    "    'indeterminate' # exam level\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df = train_df[['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID']+image_targets+exam_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_dict = dict(zip(targets_df['SOPInstanceUID'].values, targets_df[image_targets+exam_targets].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(targets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dict = defaultdict(list)\n",
    "for i,o in enumerate(files):\n",
    "    slice_no, sopid = o.stem.split(\"_\")\n",
    "    sid = o.parent.name\n",
    "    slice_no = int(slice_no)        \n",
    "    files_dict[sid].append({\"slice_no\":slice_no, \"embs_idx\":i, \"img_y\":targets_dict[sopid][0], \"exam_y\":targets_dict[sopid][1:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pids = list(files_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(pid, files_dict):\n",
    "    o = files_dict[pid]    \n",
    "    l = sorted(o, key=lambda x: x['slice_no']) \n",
    "    return tensor([o['embs_idx'] for o in l])\n",
    "\n",
    "def get_img_y(pid, files_dict):\n",
    "    o = files_dict[pid]    \n",
    "    l = sorted(o, key=lambda x: x['slice_no']) \n",
    "    img_y = [o['img_y'] for o in l]\n",
    "    img_y = tensor(img_y).float()\n",
    "    return img_y\n",
    "\n",
    "def get_exam_y(pid, files_dict):\n",
    "    d = files_dict[pid][0]        \n",
    "    exam_y = tensor(d['exam_y']).float()\n",
    "    return exam_y\n",
    "    \n",
    "# before_batch: after collecting samples before collating\n",
    "targ_pad_idx = 666\n",
    "def SequenceBlock():       return  TransformBlock(type_tfms=[partial(get_x, files_dict=files_dict)], \n",
    "                                                  dl_type=SortedDL,\n",
    "                                                  dls_kwargs={'before_batch':\n",
    "                                                               [partial(pad_input, pad_idx=input_pad_idx),\n",
    "                                                                partial(pad_input, pad_idx=targ_pad_idx, pad_fields=1)]})\n",
    "def SequenceTargetBlock(): return TransformBlock(type_tfms=[partial(get_img_y, files_dict=files_dict)])\n",
    "def TargetBlock():         return TransformBlock(type_tfms=[partial(get_exam_y, files_dict=files_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_img_y(all_pids[0], files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_exam_y(all_pids[0], files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_embeddings = F.normalize(combined_embeddings, dim=0)\n",
    "# normalized_embeddings.isnan().sum()\n",
    "# normalized_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert combined_embeddings.isnan().sum().item() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = default_device(); device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWD_LSTM(Module):\n",
    "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, emb_sz,n_hid, n_layers, hidden_p=0.2, input_p=0.6, weight_p=0.5, bidir=False):\n",
    "        store_attr('emb_sz,n_hid,n_layers')\n",
    "        self.bs = 1\n",
    "        self.n_dir = 2 if bidir else 1\n",
    "        \n",
    "        self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid)//self.n_dir, bidir, weight_p, l) for l in range(n_layers)])\n",
    "\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "        self.reset()\n",
    "\n",
    "    def forward(self, x, from_embeds=False):\n",
    "        \n",
    "        if from_embeds: inp = x\n",
    "        else: inp = combined_embeddings[x].to(device)\n",
    "        bs,sl = inp.shape[:2]\n",
    "        if bs!=self.bs: self._change_hidden(bs)\n",
    "\n",
    "        output = self.input_dp(inp)\n",
    "        new_hidden = []\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            output, new_h = rnn(output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            if l != self.n_layers - 1: output = hid_dp(output)\n",
    "        self.hidden = to_detach(new_hidden, cpu=False, gather=False)\n",
    "        return output\n",
    "\n",
    "    def _change_hidden(self, bs):\n",
    "        self.hidden = [self._change_one_hidden(l, bs) for l in range(self.n_layers)]\n",
    "        self.bs = bs\n",
    "\n",
    "    def _one_rnn(self, n_in, n_out, bidir, weight_p, l):\n",
    "        \"Return one of the inner rnn\"\n",
    "        rnn = nn.LSTM(n_in, n_out, 1, batch_first=True, bidirectional=bidir, bias=False)\n",
    "        return WeightDropout(rnn, weight_p)\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state\"\n",
    "        nh = (self.n_hid) // self.n_dir\n",
    "        return (one_param(self).new_zeros(self.n_dir, self.bs, nh), one_param(self).new_zeros(self.n_dir, self.bs, nh))\n",
    "\n",
    "    def _change_one_hidden(self, l, bs):\n",
    "        if self.bs < bs:\n",
    "            nh = (self.n_hid) // self.n_dir\n",
    "            return tuple(torch.cat([h, h.new_zeros(self.n_dir, bs-self.bs, nh)], dim=1) for h in self.hidden[l])\n",
    "        if self.bs > bs: return (self.hidden[l][0][:,:bs].contiguous(), self.hidden[l][1][:,:bs].contiguous())\n",
    "        return self.hidden[l]\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states\"\n",
    "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
    "        self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_width = 512\n",
    "layers = [lstm_width * 3] + [lstm_width] + [9]\n",
    "\n",
    "class MultiHeadedSequenceClassifier(Module):\n",
    "    \"dim: input sequence feature dim\"\n",
    "    def __init__(self, bptt=72, input_pad_idx=input_pad_idx, n_meta=1, dim=1024, nlayers=2, cls_ps=[0.4, 0.1], **awd_kwargs):\n",
    "        \n",
    "        store_attr('input_pad_idx')\n",
    "        self.awd_lstm = AWD_LSTM(dim+n_meta, lstm_width, nlayers, bidir=True, **awd_kwargs)\n",
    "#         self.awd_lstm = AWD_QRNN(dim+n_meta, 512, 2, bidir=True)\n",
    "        self.encoder = SentenceEncoder(bptt=bptt, module=self.awd_lstm, pad_idx=input_pad_idx)\n",
    "        \n",
    "        # image level preds\n",
    "        self.seq_head = LinearDecoder(1, lstm_width, bias=True)\n",
    " \n",
    "        # exam level preds\n",
    "        self.exam_head = PoolingLinearClassifier(layers, ps=cls_ps, bptt=bptt)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, mask = self.encoder(x) \n",
    "       \n",
    "        # img level out\n",
    "        seq_cls_out,_,_ = self.seq_head(out)\n",
    "        seq_cls_out = seq_cls_out.squeeze(-1)\n",
    "              \n",
    "        # exam level out\n",
    "        exam_out,_,_ = self.exam_head((out,mask))\n",
    "\n",
    "        return (seq_cls_out, exam_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLoss(Module):\n",
    "    \n",
    "    def __init__(self, targ_pad_idx=666):\n",
    "        store_attr(\"targ_pad_idx\")\n",
    "\n",
    "    def forward(self, inp, yb0, yb1):\n",
    "        image_target_weight = 0.07361963\n",
    "        exam_target_weights = tensor([0.0736196319, 0.2346625767,  0.0782208589, \n",
    "                                      0.06257668712, 0.1042944785, 0.06257668712,\n",
    "                                      0.1042944785,  0.1877300613, 0.09202453988]).to(yb1.device)\n",
    "        seq_cls_out, exam_out = inp\n",
    "       \n",
    "        # img loss\n",
    "        mask = yb0 != self.targ_pad_idx \n",
    "        \n",
    "        img_loss, qs = 0, 0        \n",
    "        for _m,_y,_p in zip(mask, yb0, seq_cls_out):\n",
    "            qi = _y[_m].mean()\n",
    "            qs += image_target_weight*qi*sum(_m)\n",
    "            img_loss += image_target_weight*qi*(F.binary_cross_entropy_with_logits(_p[_m], _y[_m], reduction='sum'))\n",
    "        \n",
    "        # exam loss\n",
    "        exam_losses = F.binary_cross_entropy_with_logits(exam_out, yb1,reduction='none')\n",
    "        tot_exam_loss = (exam_losses*(exam_target_weights.unsqueeze(0))).sum()\n",
    "        tot_exam_wgts = len(exam_losses)*(tensor(exam_target_weights).sum())\n",
    "        \n",
    "        return (tot_exam_loss+img_loss)/(qs+tot_exam_wgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLoss(Module):\n",
    "    \n",
    "    def __init__(self, targ_pad_idx=666):\n",
    "        store_attr(\"targ_pad_idx\")\n",
    "\n",
    "    def forward(self, inp, yb0, yb1):\n",
    "#         exam_targets = [\n",
    "#     'negative_exam_for_pe', # exam level\n",
    "#     'rv_lv_ratio_gte_1', # exam level\n",
    "#     'rv_lv_ratio_lt_1', # exam level\n",
    "#     'leftsided_pe', # exam level\n",
    "#     'chronic_pe', # exam level\n",
    "#     'rightsided_pe', # exam level\n",
    "#     'acute_and_chronic_pe', # exam level\n",
    "#     'central_pe', # exam level\n",
    "#     'indeterminate' # exam level\n",
    "# ]\n",
    "        image_target_weight = 0.07361963\n",
    "        exam_target_weights = tensor([0.0736196319, 0.2346625767,  0.0782208589, \n",
    "                                      0.06257668712, 0.1042944785, 0.06257668712,\n",
    "                                      0.1042944785,  0.1877300613, 0.09202453988]).to(yb1.device)\n",
    "        seq_cls_out, exam_out = inp\n",
    "       \n",
    "        # img loss\n",
    "        mask = yb0 != self.targ_pad_idx \n",
    "        \n",
    "        img_loss, qs = 0, 0        \n",
    "        for _m,_y,_p in zip(mask, yb0, seq_cls_out):\n",
    "            qi = _y[_m].mean()\n",
    "            qs += image_target_weight*qi*sum(_m)\n",
    "            img_loss += image_target_weight*qi*(F.binary_cross_entropy_with_logits(_p[_m], _y[_m], reduction='sum'))\n",
    "        \n",
    "        return (img_loss)/(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExamLoss(Module):\n",
    "    \n",
    "    def __init__(self, targ_pad_idx=666):\n",
    "        store_attr(\"targ_pad_idx\")\n",
    "\n",
    "    def forward(self, inp, yb0, yb1):\n",
    "        exam_target_weights = tensor([0.0736196319, 0.2346625767,  0.0782208589, \n",
    "                                      0.06257668712, 0.1042944785, 0.06257668712,\n",
    "                                      0.1042944785,  0.1877300613, 0.09202453988]).to(yb1.device)\n",
    "        seq_cls_out, exam_out = inp\n",
    "       \n",
    "        # exam loss\n",
    "        exam_losses = F.binary_cross_entropy_with_logits(exam_out, yb1,reduction='none')\n",
    "        tot_exam_loss = (exam_losses*(exam_target_weights.unsqueeze(0))).sum()\n",
    "        tot_exam_wgts = len(exam_losses)*(tensor(exam_target_weights).sum())\n",
    "        \n",
    "        return (tot_exam_loss)/(tot_exam_wgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_cv = True\n",
    "FOLD = fold\n",
    "\n",
    "if do_cv: \n",
    "    valid_pids = pd.read_pickle(datapath/f'cv_pids/pids_fold{FOLD}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBlock(blocks=(SequenceBlock,SequenceTargetBlock,TargetBlock), \n",
    "                 n_inp=1, \n",
    "                 splitter=FuncSplitter(lambda o: True if o in valid_pids else False)\n",
    "                )\n",
    "dls = data.dataloaders(all_pids, bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult = 1.\n",
    "cls_ps = [0.4*mult,0.1*mult]\n",
    "model = SequentialRNN(MultiHeadedSequenceClassifier(bptt=256, n_meta=2, dim=1024, nlayers=2, cls_ps=cls_ps,\n",
    "                                                    hidden_p=0.2*mult, input_p=0.6*mult, weight_p=0.5*mult)) # dim = 1536 for effnet 1024 for xresnet\n",
    "loss_func = MultiLoss()\n",
    "learner = Learner(dls, model, loss_func=loss_func, metrics=[ImageLoss(), ExamLoss()],\n",
    "                  cbs=[ModelResetter(), TerminateOnNaNCallback(),\n",
    "                       SaveModelCallback(fname=f\"nometa_sequence_with_preds_fulldata_fold{fold}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_flat_cos(20, lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get OOF Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pid(pid):\n",
    "    return pid\n",
    "\n",
    "def get_x(pid, files_dict):\n",
    "    o = files_dict[pid]    \n",
    "    l = sorted(o, key=lambda x: x['slice_no']) \n",
    "    return tensor([o['embs_idx'] for o in l])\n",
    "\n",
    "def get_img_y(pid, files_dict):\n",
    "    o = files_dict[pid]    \n",
    "    l = sorted(o, key=lambda x: x['slice_no']) \n",
    "    img_y = [o['img_y'] for o in l]\n",
    "    img_y = tensor(img_y).float()\n",
    "    return img_y\n",
    "\n",
    "def get_exam_y(pid, files_dict):\n",
    "    d = files_dict[pid][0]        \n",
    "    exam_y = tensor(d['exam_y']).float()\n",
    "    return exam_y\n",
    "    \n",
    "# before_batch: after collecting samples before collating\n",
    "targ_pad_idx = 666\n",
    "def SequenceBlock():       return  TransformBlock(type_tfms=[partial(get_x, files_dict=files_dict)], \n",
    "                                                  dl_type=SortedDL,\n",
    "                                                  dls_kwargs={'before_batch':\n",
    "                                                               [partial(pad_input, pad_idx=input_pad_idx),\n",
    "                                                                partial(pad_input, pad_idx=targ_pad_idx, pad_fields=1)]})\n",
    "def PidBlock(): return TransformBlock(type_tfms=[get_pid])\n",
    "def SequenceTargetBlock(): return TransformBlock(type_tfms=[partial(get_img_y, files_dict=files_dict)])\n",
    "def TargetBlock():         return TransformBlock(type_tfms=[partial(get_exam_y, files_dict=files_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBlock(blocks=(SequenceBlock,SequenceTargetBlock,TargetBlock, PidBlock), \n",
    "                 n_inp=1, \n",
    "                 splitter=FuncSplitter(lambda o: True if o in valid_pids else False)\n",
    "#                 splitter=RandomSplitter(0.3),\n",
    "                )\n",
    "dls = data.dataloaders(all_pids, bs=128)\n",
    "model = SequentialRNN(MultiHeadedSequenceClassifier(bptt=256, n_meta=2, dim=1024, nlayers=2)) # dim = 1536 for effnet 1024 for xresnet\n",
    "loss_func = MultiLoss()\n",
    "learner = Learner(dls, model, loss_func=loss_func, metrics=[],cbs=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load(f\"nometa_sequence_with_preds_fulldata_fold{fold}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model.eval().to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = learner.dls.test_dl(all_pids, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_pids = []\n",
    "seq_img_preds = []\n",
    "seq_img_targs = []\n",
    "seq_exam_preds = []\n",
    "seq_exam_targs = []\n",
    "with torch.no_grad():\n",
    "    for xb,yb0,yb1,pids in progress_bar(test_dl):\n",
    "        img_pred, exam_pred = to_detach(learner.model(xb))\n",
    "        seq_img_preds.append(img_pred)\n",
    "        seq_img_targs.append(yb0)\n",
    "        seq_exam_preds.append(exam_pred)\n",
    "        seq_exam_targs.append(yb1)\n",
    "        seq_pids.append(pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_datapath = datapath/'final_lstm_stacking'\n",
    "if not stacking_datapath.exists(): stacking_datapath.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolder = f\"xresnet_FOLD{FOLD}\"\n",
    "stacking_folder = stacking_datapath/subfolder\n",
    "if not stacking_folder.exists(): stacking_folder.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seq_pids), len(seq_img_preds), len(seq_img_targs), len(seq_exam_preds), len(seq_exam_targs), len(valid_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(seq_pids, stacking_folder/'seq_pids.pth')\n",
    "torch.save(seq_img_preds, stacking_folder/'seq_img_preds.pth')\n",
    "torch.save(seq_img_targs, stacking_folder/'seq_img_targs.pth')\n",
    "torch.save(seq_exam_preds, stacking_folder/'seq_exam_preds.pth')\n",
    "torch.save(seq_exam_targs, stacking_folder/'seq_exam_targs.pth')\n",
    "torch.save(valid_pids, stacking_folder/'valid_pids.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
