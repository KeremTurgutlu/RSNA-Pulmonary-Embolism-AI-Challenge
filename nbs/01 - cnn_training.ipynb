{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path(\"../rsna_data/\")\n",
    "train_df = pd.read_csv(datapath/'train.csv')\n",
    "train_df.pe_present_on_image.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_sample=False\n",
    "if do_sample:\n",
    "    sample_pids_dir = (datapath/'sample_pids')\n",
    "    valid_pids = pd.read_pickle(sample_pids_dir/'valid_pids.pkl')\n",
    "    train_pids = pd.read_pickle(sample_pids_dir/'train_pids.pkl')\n",
    "\n",
    "    len(train_pids), len(valid_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_cv = True\n",
    "FOLD = 0\n",
    "\n",
    "if do_cv: \n",
    "    cv_pids_dir = (datapath/'cv_pids')\n",
    "    if not cv_pids_dir.exists(): cv_pids_dir.mkdir()\n",
    "    cv_df = train_df[['StudyInstanceUID', 'negative_exam_for_pe']].drop_duplicates().reset_index(drop=True)\n",
    "    all_pids = cv_df['StudyInstanceUID'].values\n",
    "\n",
    "    valid_pids = pd.read_pickle(datapath/f'cv_pids/pids_fold{FOLD}.pkl')\n",
    "    train_pids = list(set(all_pids).difference(valid_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_pids), len(valid_pids), len(train_pids+valid_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = datapath/'metadata'\n",
    "train_metadata_paths = [o for o in metadata_path.ls() if o.stem in train_pids]\n",
    "valid_metadata_paths = [o for o in metadata_path.ls() if o.stem in valid_pids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_metadata_paths), len(valid_metadata_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline valid performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadf = pd.concat([pd.read_csv(o) for o in train_metadata_paths]).reset_index(drop=True)\n",
    "valid_metadf = pd.concat([pd.read_csv(o) for o in valid_metadata_paths]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadf.shape, valid_metadf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadf = train_metadf.merge(train_df, on=['StudyInstanceUID', 'SOPInstanceUID'])\n",
    "valid_metadf = valid_metadf.merge(train_df, on=['StudyInstanceUID', 'SOPInstanceUID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadf['pe_present_on_image'].mean(), valid_metadf['pe_present_on_image'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = valid_metadf['pe_present_on_image'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(p*np.log(p)+(1-p)*np.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targs = tensor(valid_metadf['pe_present_on_image'].values)\n",
    "preds = torch.ones_like(targs)*p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.binary_cross_entropy(preds,targs.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.stack([torch.ones_like(targs)*(1-p), torch.ones_like(targs)*(p)],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_ce = F.nll_loss(torch.log(preds), targs); baseline_ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_acc = 1 - p; baseline_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1-p)/p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = dict(zip(train_df['SOPInstanceUID'], train_df['pe_present_on_image']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels_dict), len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdatapath = (datapath/'full_raw_512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_image_files(imgdatapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [o for o in files if o.parent.name in train_pids]\n",
    "valid_files = [o for o in files if o.parent.name in valid_pids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_files), len(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart Sample\n",
    "\n",
    "We don't have to use all the slices as input data per patient. We can simply sample every nth slice for each patient so that we have good enough variability within that patient and use that data for CNN feature training. But always keep, if y = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.medical.imaging import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dict = defaultdict(list)\n",
    "for o in files:\n",
    "    files_dict[o.parent.name].append(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in files_dict:\n",
    "    files_dict[k] = sorted(files_dict[k], key=lambda o: int(o.name.split('_')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nslices = [len(files_dict[k]) for k in files_dict]\n",
    "plt.hist(nslices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_patient_slices(pid, num_slice_samples):\n",
    "    \"Use a fixed number of samples per patient for training speed up\"\n",
    "    files = array(files_dict[pid])\n",
    "    n = len(files)\n",
    "    if n > num_slice_samples:\n",
    "        labels = [labels_dict[o.stem.split(\"_\")[1]] for o in files]\n",
    "        pos_idxs = list(np.where(labels)[0])\n",
    "        idxs = [np.clip(int(i), 0, n-1) for i in np.linspace(0, n, num_slice_samples)]\n",
    "        idxs += pos_idxs\n",
    "        idxs = sorted(set(idxs))\n",
    "        return files[idxs]\n",
    "    else: return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampled_files = parallel(partial(sample_patient_slices, num_slice_samples=200), train_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = []\n",
    "for o in train_sampled_files: train_files += list(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_files = []\n",
    "for o in valid_pids: valid_files += files_dict[o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_files), len(valid_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_transforms(mult=1.0, do_flip=True, flip_vert=False, max_rotate=10., min_zoom=1., max_zoom=1.1,\n",
    "                   max_lighting=0.2, max_warp=0.2, p_affine=0.75, p_lighting=0.75, xtra_tfms=[], size=None,\n",
    "                   mode='bilinear', pad_mode=PadMode.Reflection, align_corners=True, batch=False, min_scale=1.):\n",
    "    \"Utility func to easily create a list of flip, rotate, zoom, warp, lighting transforms.\"\n",
    "    res,tkw = [],dict(size=size if min_scale==1. else None, mode=mode, pad_mode=pad_mode, batch=batch, align_corners=align_corners)\n",
    "    max_rotate,max_lighting,max_warp = array([max_rotate,max_lighting,max_warp])*mult\n",
    "    if do_flip: res.append(Dihedral(p=0.5, **tkw) if flip_vert else Flip(p=0.5, **tkw))\n",
    "    if max_warp:   res.append(Warp(magnitude=max_warp, p=p_affine, **tkw))\n",
    "    if max_rotate: res.append(Rotate(max_deg=max_rotate, p=p_affine, **tkw))\n",
    "    if min_zoom<1 or max_zoom>1: res.append(Zoom(min_zoom=min_zoom, max_zoom=max_zoom, p=p_affine, **tkw))\n",
    "    if max_lighting:\n",
    "        res.append(Brightness(max_lighting=max_lighting, p=p_lighting, batch=batch))\n",
    "        res.append(Contrast(max_lighting=max_lighting, p=p_lighting, batch=batch))\n",
    "    if size: xtra_tfms += [RandomResizedCropGPU(size, min_scale=min_scale, ratio=(1,1))]\n",
    "    return res + L(xtra_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgtdict = {0:1, 1:10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(o): \n",
    "    return labels_dict[o.stem.split(\"_\")[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlipUD(RandTransform):\n",
    "    def __init__(self, p=0.5): super().__init__(p=p)\n",
    "    def encodes(self, x:TensorImage): return x.flip(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def get_dls(train_files, valid_files, resize=256, size=224, bs=128):\n",
    "    \n",
    "    files = train_files + valid_files\n",
    "    trn_wgts = [wgtdict[get_label(o)] for o in train_files]\n",
    "    print(f\"Collected idxs\")\n",
    "\n",
    "    tfms = [[PILImage.create, ToTensor, RandomResizedCrop(resize, min_scale=0.9)], \n",
    "            [get_label, Categorize()]]\n",
    "    dsets = Datasets(files, tfms=tfms,\n",
    "                     splits=FuncSplitter(lambda o: True if o.parent.name in valid_pids else False)(files))\n",
    "    print(f\"Created dset\")\n",
    "\n",
    "    aug_tfms = aug_transforms(size=size, max_lighting=False, max_warp=False, flip_vert=False, min_scale=0.75,\n",
    "                              xtra_tfms=[\n",
    "                                      RandomErasing(p=0.25, sh=0.2, min_aspect=0.15), \n",
    "                                        ])    \n",
    "    batch_tfms = [IntToFloatTensor] + aug_tfms\n",
    "    dls = dsets.dataloaders(bs=bs, after_batch=batch_tfms, dl_type=WeightedDL, dl_kwargs=[{\"wgts\":trn_wgts}, {}])\n",
    "    print(f\"DLs ready\")\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = partial(ranger, **dict(sqrmom=0.99, mom=0.95, beta=0., eps=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progressive Resizing\n",
    "resize, size = (512, 480)\n",
    "# resize, size = (256, 224)\n",
    "dls = get_dls(train_files, valid_files, resize=resize, size=size, \n",
    "#               bs=int(64*(256/resize))\n",
    "              bs=32\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = LabelSmoothingCrossEntropyFlat(eps=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = cnn_learner(dls, xresnet34, opt_func=opt_func, \n",
    "#                     pretrained=False, loss_func=loss_func,\n",
    "#                     metrics=[accuracy], cbs=[SaveModelCallback(\"accuracy\", \n",
    "#                                                                fname=f\"xresnet34-{resize}-PR-fold{FOLD}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_name('efficientnet-b3', num_classes=2)\n",
    "learn = Learner(dls, model, opt_func=opt_func, \n",
    "                    pretrained=False, loss_func=loss_func,\n",
    "                    metrics=[accuracy], cbs=[SaveModelCallback(\"accuracy\", \n",
    "                                                               fname=f\"effb3-{resize}-PR-fold{FOLD}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.dls.bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpth = Path(f\"effb3-256-PR-fold{FOLD}\")\n",
    "learn.load(loadpth)\n",
    "print(\"loaded...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.to_fp16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_flat_cos(3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.export(f\"./models/xresnet34-{resize}-PR-fold{FOLD}-export.pkl\");\n",
    "learn.export(f\"./models/effb3-{resize}-PR-fold{FOLD}-export.pkl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.print_classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(max_n=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb=learn.dls.train.one_batch()\n",
    "yb.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(interp.preds[:,1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate OOF Preds & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
